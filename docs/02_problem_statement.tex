Решается задача \textit{детоксификации} текстов.
Требуется по токсичному предложению построить его нейтральный вариант, который сохраняет семантический смысл.  
Под токсичностью в большинстве случаев подразумевается наличие обсценной лексики.

\subsection{Детоксификация, как машинный перевод}
\label{section:machine_translation}
Задачу детоксификации можно рассматривать, как задачу переноса текстового стиля. 
Но в данной работе имеется параллельный корпус текстов, то сперва удобнее рассматривать задачу, как задачу машинного перевода (seq-to-seq). 

Данные представляют собой параллельный корпус текстов $(s_t, s_d)$, где $s_t$~--- токсичное предложение из датасета, $s_d$~--- его нейтральная версия. 
Обозначим два пространства с различными стилями: $T$~--- пространство токсичных предложений, $D$~--- пространство нейтральных предложений.
Тогда требуется построить модель из $T$ в $D$.

Для работы нейросетевых моделей с текстом требуется разбить его на токены. 
В данной работе для разбиения текста на токены используется BPE-токенизатор \cite{bpe}. 

Пускай имеется \textit{словарь} $V_f$, содержащий все возможные токены для модели-детоксификатора.
Тогда $\tau_f$~--- \textit{токенизатор}, такой что $$\tau_f: T \cup D \to \left(V_f\right)^{n},$$ где $n$~--- фиксированная длина последовательности токенов.  
Причём в данной постановке задачи токенизатор  $\tau_{f}$ считается неизменным, и берётся уже обученным с фиксированным словарём $V_{f}$. 
Пусть также имеется два предложения $s_t \in T$ и $s_d \in D$.
Тогда $t = \tau_{f}(s_t), d = \tau_{f}(s_d)$~--- их разбиения на токены.
Введём \textit{модель-детокисфикатор} $f_{\theta}$, такую что 
\begin{gather*}
    f_{\theta}: \left(V_f\right)^{n} \to [0, 1]^{n \times |V_f|}, \\
    f_{\theta}(* | d_{<i}, t) \in [0, 1]^{|V_f|},
\end{gather*}
где $f_{\theta}(* | d_{<i}, t)$~--- распределение вероятностей $i$-го токена при условии токсичного предложения и сгенерированных $i-1$ токене.
% Обозначим за $p \in [0, 1]^{n \times |V_f|}$~--- вероятности истинных токенов $d$.
% Причём это one-hot вектора, то есть $p_{*, i} = 1$.

В простейшей постановке задачи машинного перевода имеется функция потерь задаётся кросс-энтропией:
\begin{gather*}
    \mathcal{L}_{\text{CE}} = -\sum_{i=1}^{n} \log f_\theta(d_{i} | d_{<i}, t).
    % \mathcal{L}_{\text{CE}}(p^*, p) 
    % = -\sum_{j=1}^{n} \sum_{v=1}^{|V|} p^{*}_{j, v} \log p_{j, v} 
    % = -\sum_{j=1}^{n} \log \left(p(d_{j} | d_{<j}, t) \right).
\end{gather*}
Тогда решаемая задача записывается как:
\begin{gather*}
    \mathcal{L}_{\text{CE}} \longrightarrow \min_{\theta}.
\end{gather*}


\subsection{Детоксификация, как текстовая стилизация}
Отметим, что в задаче машинного перевода кросс-энтропийная функция потерь никак не учитывает нейтральность полученных предложений, то есть не учитывается задача стилизации.
Это распространённая проблема в задачах переноса текстового стиля при наличии параллельного корпуса текстов \cite{Tikhonov2018WhatIW}.
Один из возможных способ учитывать нейтральность полученного предложения~--- это подсчёт потенциально токсичных и оскорбительных слов. 
При таком подходе возникает много трудностей.
Требуется учитывать различную морфологию, семантику, контекст, а также иметь в наличии список потенциально токсичных и оскорбительных слов. 
Причём такой подход не будет являться дифференцируемым.
Поэтому, учитывая результаты нейросетевых моделей в задачах классификации, наиболее эффективным подходом оценки степени токсичности являются нейросети.

Обозначим нейросетевую модель, отвечающую за оценку токсичности предложения, как $g_{\text{toxic}}$.
Далее будем называть её \textit{оценочной моделью}.
Как и модели-детоксификатора, оценочная модель имеет фиксированный словарь $V_{g}$ и фиксированный обученый BPE-токенизатор $\tau_{g}: T \cup D \to \left(V_{g}\right)^{n}$.
Тогда можно записать: 
\begin{gather*}
    g_{\text{toxic}}:\left(V_{g}\right)^{n} \to [0, 1].
\end{gather*}
Будем рассматривать выход модель, как вероятность токсичности предложения. 
Введём детоксифицированное предложение $s_{\text{detox}}$, полученное моделью-детоксификатором для входного токсичного предложения $s_t$, 
\begin{gather*}
    s_{\text{detox}} = \{\arg\max_{d_i} f_{\theta}(d_{i} | d_{<i}, t)\}_{i=1}^{n}.
\end{gather*}
Тогда решение следующей задачи будет явно учитывать задачу переноса стиля: 
\begin{gather*}
    \mathcal{L}_{\text{TP}} (s_{\text{detox}}) = g_{\text{toxic}} \bigl(\tau_{g} (s_{\text{detox}}) \bigr) \longrightarrow \min_{\theta}.   
\end{gather*}

\subsection{Проблема}
\label{section:problem_statement}
При использовании оценочной модели $g_{\text{toxic}}$ в качестве функции потерь возникает проблема отсутствия дифференцируемости по параметрам детоксификатора $\theta$.
Дифференцируемость нарушается при построении $s_{\text{detox}}$ из-за операции $\arg\max$, а также по причине различных токенизаторов у детоксификатора и оценочной модели.
Вторая проблема в первую очередь возникает из-за различных словарей $V_{f}, V_{g}$.
Нет гарантий, что существует инъективное отображение между токенами $\tau_{f}$ и $\tau_{g}$. 
Проще всего увидеть проблему на примере: 

\begin{gather*}
    \textit{ты слишком токсичный} \longrightarrow 
    \begin{cases} 
        [\textit{ты},\ \textit{слишком},\ \textit{токс},\ \textit{ич},\ \textit{ный}], &\text{от}\ \tau_{f}, \\
        [\textit{ты},\ \textit{\#UNK},\ \textit{токсичн},\ \textit{ый}], &\text{от}\ \tau_{g}.
    \end{cases}
\end{gather*}
В последующей главе  будет предложен метод позволяющий обойти описанные выше проблемы. 
