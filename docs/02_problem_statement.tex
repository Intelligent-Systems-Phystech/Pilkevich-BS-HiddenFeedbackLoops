\subsection{Детоксификация, как машинный перевод}
\label{section:machine_translation}
В данной работе рассматривается задача \textit{детоксификации} текста. 
Требуется по токсичному предложение построить нейтральный без потери семантического смысла. 
Под токсичным предложением может подразумеваться как предложение с обсценной лексикой, так и предложения оскорбительного характера без её употребления.

На этапе обучения доступен корпус из токсичных предложений, для каждого из которых известен нейтральный вариант.
Примеры приведены в таблице $\ref{table:dataset_examples}$.
Поэтому задачу детоксификации удобно рассматривать, как задачу машинного перевода (seq-to-seq). 

Данные представляют собой параллельный корпус текстов $(s_t, s_d)$, где $s_t$~--- токсичное предложение из датасета, $s_d$~--- его нейтральная версия. 
Обозначим два пространства с различными стилями: $T$~--- пространство токсичных предложений, $D$~--- пространство нейтральных предложений.
Тогда требуется построить отображение из $T$ в $D$.

Для работы нейросетевых моделей с текстом требуется разбить его на токены. 
В данной работе для разбиения текста на токены используется BPE-токенизатор \cite{bpe}. 

Пускай имеется \textit{словарь} $V_f$, содержащий все возможные токены для модели-детоксификатора.
Тогда $\tau_f$~--- \textit{токенизатор}, такой что $$\tau_f: T \cup D \to \left(V_f\right)^{n},$$ где $n$~--- фиксированная длина последовательности токенов.  
Следует заметить, что в данной постановке задачи токенизатор $\tau_{f}$ и словарь $V_{f}$ не дообучаются под задачу детоксификации, а являются фиксированными. 
Значения $\tau_{f}$ и $V_{f}$ зависят только от предобученной модели, использующейся для инициализации весов целевой модели.

Пусть также имеется два предложения $s_t \in T$ и $s_d \in D$.
Тогда $t = \tau_{f}(s_t), d = \tau_{f}(s_d)$~--- их разбиения на токены.
Введём \textit{детокисфикатор} $f_{\theta}$, такую что 
\begin{gather*}
    f_{\theta}: \left(V_f\right)^{n} \to [0, 1]^{n \times |V_f|}, \\
    f_{\theta}(* | d_{<i}, t) \in [0, 1]^{|V_f|}, \\
    \sum_{d_i \in V_f} f_\theta(d_i | d_{<i}, t) = 1,
\end{gather*}
где $f_{\theta}(* | d_{<i}, t)$~--- распределение вероятностей $i$-го токена при условии токсичного предложения и сгенерированных $i-1$ токене.
В данной работе детоксификатор имеет архитектуру кодировщик-декодировщик, трансформер T5\cite{t5}. 
Как отмечалось ранее, параметры детоксификатора будут инициализироваться параметрами модели предобученной на другую задачу. 
Мотивация такой инициализации заключается в том, что предобученная модель обучалась на корпусе текстов в разы больше и у неё сформировалось <<понимание>> языка.
Подход дообучения модели хорошо себя зарекомендовал, и позволяет качественно решать поставленную задачу при наличии маленького корпуса обучающих данных\cite{sun2019fine}.  
Более детально о выбранной модели будет описано в секции $\ref{section:baseline}$.

Детоксификатор, как модель кодировщик-декодировщик, генерирует токены в авторегрессионном подходе.
Токены генерируются один за одним, на каждом шаге выбирая токен с наибольшей вероятностью при условии предыдущих: 
\begin{gather*}
    \arg\max_{d_{i} \in V_f} f_{\theta}(d_i) | d_{<i}, t).
\end{gather*}
Заметим, что после генерации токена, означающего конец предложения, оставшиеся токены будут не информативными и заполняются специальным токеном. 
После генерации последовательности, она передаётся в функцию \textit{детокенизации} $\text{Text}_f$, которая составляет текст $s_{\text{detox}}$ на основе последовательности токенов и словаря детоксификатора $V_{f}$:
\begin{gather*}
    s_{\text{detox}} = \text{Text}_{f}\left( \{\arg\max_{d_i} f_{\theta}(d_{i} | d_{<i}, t)\}_{i=1}^{n} \right).
\end{gather*}
% Обозначим за $p \in [0, 1]^{n \times |V_f|}$~--- вероятности истинных токенов $d$.
% Причём это one-hot вектора, то есть $p_{*, i} = 1$.

В простейшей постановке задачи машинного перевода имеется функция потерь задаётся кросс-энтропией:
\begin{gather*}
    \mathcal{L}_{\text{CE}} = -\sum_{i=1}^{n} \log f_\theta(d_{i} | d_{<i}, t).
    % \mathcal{L}_{\text{CE}}(p^*, p) 
    % = -\sum_{j=1}^{n} \sum_{v=1}^{|V|} p^{*}_{j, v} \log p_{j, v} 
    % = -\sum_{j=1}^{n} \log \left(p(d_{j} | d_{<j}, t) \right).
\end{gather*}
Тогда решаемая задача записывается как:
\begin{gather*}
    \mathcal{L}_{\text{CE}} \longrightarrow \min_{\theta}.
\end{gather*}


\subsection{Детоксификация, как текстовая стилизация}
Отметим, что в задаче машинного перевода кросс-энтропийная функция потерь явно не учитывает нейтральность полученных предложений, то есть не учитывается задача стилизации.
Это распространённая проблема в задачах переноса текстового стиля при наличии параллельного корпуса текстов \cite{Tikhonov2018WhatIW}.
Один из возможных способ учитывать нейтральность полученного предложения~--- это подсчёт потенциально токсичных и оскорбительных слов. 
При таком подходе возникает много трудностей.
Требуется учитывать различную морфологию, семантику, контекст, а также иметь в наличии список потенциально токсичных и оскорбительных слов. 
Поэтому, учитывая успехи нейросетевых моделей в задачах классификации\cite{XLNet, devlin-etal-2019-bert}, наиболее эффективным подходом оценки степени токсичности являются нейросети.

Обозначим нейросетевую модель, отвечающую за оценку токсичности предложения, как $g_{\text{toxic}}$.
Далее будем называть её \textit{оценочной моделью}.
Как и модели-детоксификатора, оценочная модель имеет фиксированный словарь $V_{g}$ и фиксированный обученый BPE-токенизатор $\tau_{g}: T \cup D \to \left(V_{g}\right)^{n}$, необязательно совпадающие с $V_{f}, \tau_{f}$.
С учётом приведённых выше обозначений и шкалы токсичности от нуля до одного можем записать: 
\begin{gather*}
    g_{\text{toxic}}:\left(V_{g}\right)^{n} \to [0, 1].
\end{gather*}
Будем рассматривать выход модели, как вероятность токсичности предложения. 
Введём детоксифицированное предложение $s_{\text{detox}}$, полученное моделью-детоксификатором для входного токсичного предложения $s_t$, как это было описано в предыдущей секции $\ref{section:machine_translation}$.

Тогда решение следующей задачи будет явно учитывать задачу переноса стиля: 
\begin{gather*}
    \mathcal{L}_{\text{TP}} (s_{\text{detox}}) = g_{\text{toxic}} \bigl(\tau_{g} (s_{\text{detox}}) \bigr) \longrightarrow \min_{\theta}.   
\end{gather*}

\subsection{Проблема отсутствия дифференцируемости}
\label{section:problem_statement}
Как было описано в секции $\ref{section:machine_translation}$, для лучшего решения задачи берётся инициализация параметров детоксификатора предобученной моделью. 
Это приводит к тому, что токенизатор $\tau_{f}$ и словарь $V_{f}$ являются фиксированными, так как иначе не будет эффекта от инициализации. 
В качестве оценочной модели будет использоваться обученная модель, которая также имеет фиксированный токенизатор $\tau_{g}$ и словарь $V_{g}$, которые скорее всего отличны от $\tau_{f}, V_{f}$.

При использовании оценочной модели $g_{\text{toxic}}$ в качестве функции потерь возникает проблема отсутствия дифференцируемости по параметрам детоксификатора $\theta$.
Дифференцируемость нарушается при построении $s_{\text{detox}}$ из-за операции $\arg\max$, а также по причине различных токенизаторов у детоксификатора и оценочной модели.
Вторая проблема в первую очередь возникает из-за различных словарей $V_{f}, V_{g}$.
Нет гарантий, что существует инъективное отображение между токенами $\tau_{f}$ и $\tau_{g}$. 
Проще всего увидеть проблему на примере: 
\begin{gather*}
    \textit{ты слишком токсичный} \longrightarrow 
    \begin{cases} 
        [\textit{ты},\ \textit{слишком},\ \textit{токс},\ \textit{ич},\ \textit{ный}], &\text{от}\ \tau_{f}, \\
        [\textit{ты},\ \textit{\#UNK},\ \textit{токсичн},\ \textit{ый}], &\text{от}\ \tau_{g}.
    \end{cases}
\end{gather*}

В случае одинаковых словарей проблема отображения между токенами решилась автоматически, так как используются одинаковые токены. 
А проблему не дифференцируемости операции $\arg\max$ можно решить, например, при использовании gumbel-softmax или других подходов. 
Для случая наличия инъективного отображения между словарём $V_{f}$ и $V_{g}$ всё сведётся к дифференцируемости $\arg\max$, так как мы просто не будем использовать часть токенов $V_{g}$, которые нам не нужны.

В последующей главе  будет предложен метод позволяющий обойти проблему дифференцируемости $\arg\max$ и различия словарей $V_{f}, V_{g}$. 
