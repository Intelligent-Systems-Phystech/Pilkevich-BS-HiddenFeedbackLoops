\begin{abstract}

Часто нейросетевые модели используются в качестве метрик качества для задач обработки естественного языка.
Возникает желание использовать их в качестве функции потерь, чтобы явно оптимизировать заданную метрику. 
Но в силу их неизменяемости появляется проблема с дифференцируемостью такой функции потерь, так как различные токенизаторы нарушают её.
В первую очередь это связано с тем, что отсутствует инъективное отображение между токенами двух токенизаторов: модели, решающей задачу, и оценочной модели.  
Данная работа предлагает способ решения отсутствия дифференцируемости такой функции потерь на примере задачи детоксификации текстов.  
Предлагается обучить новый входной эмбеддинг-слой оценочной модели, который будет принимать распределения вероятностей токенов, выданных моделью детоксификатором. 
Это позволит использовать её в качестве функции потерь.
В работе описывается алгоритм одновременного обучения детоксификатора и адаптера.
Также приводятся эксперименты доказывающие эффективность предложенного метода.

\bigskip
\noindent

\textbf{Ключевые слова}: \emph{deep learning, natural language processing, text style transfer.}
\end{abstract}