\subsection{Данные}
Данные взяты из соревнования \textit{"Russian Text Detoxification Based on Parallel Corpora"} \cite{russe_data}.
Предложения были собраны с таких платформ, как 
"Одноклассники"\footnote{\url{https://www.kaggle.com/datasets/alexandersemiletov/toxic-russian-comments}}, 
"Pikabu"\footnote{\url{https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments}}, 
"Twitter"\footnote{\url{http://study.mokoron.com/}}.
Как было описано раннее, датасет представляет себе параллельный корпус русскоязычных предложений, состоящий из токсичного варианта и его перефразированного нейтрального варианта. 
Нейтральная версия токсичного предложения была получена при помощи толокерской разметки\footnote{\url{https://toloka.yandex.ru/}}. 
У каждого токсичного предложения имеется от одного до трёх нейтральных вариантов. 

Размер обучающего датасета составляет $N_{\text{train}} = 11136$ пар $\left(s_t, s_d \right)$.
Из которых $10\%$ использовались для валидации во время обучения.
Размер тестового датасета составляет $N_{\text{test}} = 800$ предложений.
Примеры: 
\begin{table}[ht]
\centering
 \begin{tabular}{|l|l|} 
 \hline
 Токсичное предложение & Нейтральный вариант \\ [0.5ex] 
 \hline
 \textit{<<Её муженька козла на кол} & \textit{<<Её мужа нужно наказать.>>} \\
 \textit{надо посадить.>>} &  \\
 \textit{<<Это твари а не люди>>} & \textit{<<Это плохие люди.>>} \\
 \textit{<<Да такой же урод и выдал.>>} & \textit{<<Этот же человек и посоветовал.>>} \\
 \hline
 \end{tabular}
\end{table}

\subsection{Метрики}
В качестве метрик для оценки качества детоксификации использовались chrF\cite{popovic-2015-chrf}, Style transfer accuracy (STA) и их произведение.

\textbf{ChrF}~--- F-score на основе символьных $n$-грамм.
Отвечает за оценку качества <<перевода>> по сравнению с истинным предложения.
Введём chrP~--- доля символьных n-грамм из предлагаемого предложения, которые имеются в оригинальном.
И chrR~--- доля символьных n-грамм из оригинального предложения, которые представлены в предлагаемом. 
Тогда финальная формула:
\begin{gather*}
    \text{chrF}_{\beta} = (1 + \beta^2) \frac{\text{chrP} \cdot \text{chrR}}{\beta^2 \text{chrP} + \text{chrR}},
\end{gather*}
где chrP, chrR считаются как среднее по всем предложениям. 

\textbf{Style transfer accuracy (STA)}~--- вероятность токсичности предложения.
Метрика задаётся оценочной моделью $g_{\text{toxic}}$.
В качестве модели $g_{\text{toxic}}$ используется предобученный Conversational RuBERT, дообученный на задачу определения токсичности\footnote{\url{https://huggingface.co/SkolkovoInstitute/russian_toxicity_classifier}}. 

Тем самым chrF1 отвечает за сохранения смысла по сравнению с истинным нейтральным предложением, а STA отвечает за перенос стиля. 
В качестве целевой метрики используется произведение chrF1 и STA, дабы учесть влияние каждой из задач.

\subsection{Baseline T5}
\label{section:baseline}
Как было описано в главе $\ref{section:machine_translation}$, при наличии параллельного корпуса текстов задачу текстовой стилизации можно решать, как задачу машинного перевода. 
Поэтому в качестве исходной точки используется именно такой подход, чтобы была возможность оценивать эффект предложенных подходов.

В качестве детоксификатора $f_{\theta}$ используется трансформер T5\cite{t5}, то есть архитектура кодировщик-декодировщик.
Так как размер датасета в данной работе мал, то наиболее выгодным решением является использование предобученную архитектуры и дообучение её на задачу детоксификации.
В качестве предобученной модели берётся ruT5-base\footnote{\url{https://huggingface.co/sberbank-ai/ruT5-base}}, обученная на задаче seq-to-seq при испольховании датасет размером $300$Gb. 

\subsection{Одновременное обучение адаптера и детоксификатора}
В этой и следующей главе в качестве исходной модели детоксификатора $f_{\theta}$ используется модель полученная в предыдущей главе $\ref{section:baseline}$, после дообучения на задачу детоксификации, как на задачу машинного перевода.

Изначальная идея метода заключалась в одновременном обучении адаптера $A$ и модели детоксификатора $f_{\theta}$. 
Причём адаптер на вход принимал выходные эмбеддинги модели детоксификатора.
Решаемую в это случае задачу можно записать через агрегирующую функцию потерь $\mathcal{F}$ следующим образом:
\begin{gather*}
    \mathcal{F} (\text{CE, TP}) \longrightarrow \min_{\theta, A}.
\end{gather*}
Но такой подход естественным образом приводит к переобучению адаптера.
Адаптеру выгодно возвращать "шум" для оценочной модели $g_{\text{toxic}^{*}}$, чтобы уменьшить значения стилизационной функции потерь $\mathcal{L}_{\text{TP}}$. 
Как видно из таблицы $\ref{table:same_time_training}$, данный подход приводил к переобучению.
Целевая метрика STA*chrF1 ухудшалась на $8\%$, что сильнее всего связано с уменьшением STA-метрики на $10\%$. 

Следующая идея заключалась в использование softmax от выходных логитов модели детоксификатора $f_{\theta}$ в качестве входов адаптера при обучении на одинаковую функцию потерь $\mathcal{F}$. 
В этом случае выход детоксификатора можно интерпретировать, как one-hot вектора детоксификатора, как это было описано в главе $\ref{section:adapter_defenition}$.
Но также, как и в предыдущем эксперименте, такой подход обучения детоксификатора приводит к переобучению модели, что видно из результатов в таблице $\ref{table:same_time_training}$.
При этом результат получается лучше, чем при использовании эмбеддингов, что связано с меньшей "свободой" для адаптера и возможностей для переобучения. 

В обоих подходов в качестве агрегирующей функции потерь $\mathcal{F}$ использовалось произведение: $\mathcal{F}(\text{CE, TP}) = \text{CE} \cdot \text{TP}$. 
Данный выбор был обусловлен тем, что целевой метрикой является STA*ChrF1.

\begin{table}[ht]
\centering
 \begin{tabular}{|l l|c c c|} 
 \hline
 Подход & $\mathcal{F}$ & STA & chrF1 & STA*chrF1 \\ [0.5ex] 
 \hline
 seq-to-seq & CE & 0.742 & 0.577 & 0.428  \\ 
 Adapter on embs & CE $\cdot$ TP & 0.639 & 0.544 & 0.348 \\
 Adapter on logits  & CE $\cdot$ TP & 0.708 & 0.569 & 0.403 \\
  \hline
 \end{tabular}
\caption{Результаты подхода, когда детоксификатор и адаптер оптимизируют общую функцию потерь $\mathcal{F}$.}
\label{table:same_time_training}
\end{table}

\subsection{Удачные эксперименты}
Следующий подход заключался в раздельном обучении модели детоксификатора и адаптера при использовании различных функций потерь. 
Сперва обучался адаптер $A$ также, как это было описано в главе $\ref{section:adapter_training}$. 
Но далее, в отличии от финальной версии предложенного метода, на одном и том же батче сперва обучался детоксификатор при фиксированном адаптаре на $\mathcal{F}$, а потом дообучался адаптер при фиксированном детоксификаторе на $D_{\text{KL}}$.
Далее такой подход будем называть как \textit{Same batch}.

В качестве функции потерь $\mathcal{F}(\text{CE}, \text{TP})$ использовались два варианта: 
\begin{gather*}
    \mathcal{F}(\text{CE}, \text{TP}) = \text{CE} \cdot \text{TP}, \\
    \mathcal{F}(\text{CE}, \text{TP}) = w_{1} \cdot \text{CE} + w_{2} \cdot \text{TP}. 
\end{gather*}
Наилучшим образом себя показали коэффициенты для взвешенной функции потерь: $w_1= 1, w_2 = 10$. Это объясняется тем, что модель детоксификатор до этого уже была обучена на кросс-энтропию и важно обращать внимание на функцию потерь, отвечающую за стилизацию, и взвешенная функция потерь позволяет сделать это явно.
В данной постановке обучения проводились три запуска каждого из экспериментов с различными seed.
Как видно из таблицы $\ref{table:same_batch}$, есть значимое улучшение STA на $~3\%$, но при этом происходит уменьшение chrF1. 
В результате чего целевая метрика в виде их произведение почти не изменяется.
При этом использование различных функций потерь $\mathcal{F}$ даёт практически одинаковый результат.   
\begin{table}[ht]
\centering
 \begin{tabular}{|l l|c c c|} 
 \hline
 Подход & $\mathcal{F}$ & STA & chrF1 & STA*chrF1 \\ [0.5ex] 
 \hline
 seq-to-seq & CE & $0.744 \pm 0.010$ & $0.575 \pm 0.002$ & $0.430 \pm 0.042$ \\ 
 Same batch & $\text{CE} \cdot \text{TP}$ & $0.776 \pm 0.015$ & $0.569 \pm 0.004$ & $0.442 \pm 0.006$ \\
 Same batch & $\text{CE} + 10 \cdot \text{TP}$ & $0.774 \pm 0.011$ & $0.561 \pm 0.012$ & $0.435 \pm 0.013$ \\
  \hline
 \end{tabular}
\caption{Эксперименты по проверке статистической значимости подхода с обучением детоксификатора и адаптера на одном батче при использовании разных функций потерь, Same batch.}
\label{table:same_batch}
\end{table}

После этого использовалось финальный подход обучения, предложенная в главе $\ref{section:posttrain}$.
Будет называть его \textit{GAN style}.
Также как и в предыдущем эксперименте использовались два различных варианта функции потерь $\mathcal{F}$:
\begin{gather*}
    \mathcal{F}(\text{CE}, \text{TP}) = \text{CE} \cdot \text{TP}, \\
    \mathcal{F}(\text{CE}, \text{TP}) = w_{1} \cdot \text{CE} + w_{2} \cdot \text{TP}. 
\end{gather*}
При таком подходе обучения наилучшим образом себя показал вариант с взвешенной функцией потерь и явным предпочтением для стилизационной функции потерь: $w_{1} = 1, w_{2} = 10$. 
Результаты отображены в таблице $\ref{table:final_exp}$.
Удалось достичь улучшения STA на $7.4\%$ и STA*chrF1 на $3.5\%$.

Также проводился эксперимент с дообучение детоксификатора только на стилизационную функцию потерь TP. 
При таком подходе происходит явное переобучение, что видно из метрик: STA $\approx 1.0$ и ChrF1 в $5$ раз меньше. 
Если посмотреть на сгенерированные предложения, то окажется, что детоксификатор выдаёт одно и тоже предложение для любого входа.

\begin{table}[ht]
\centering
 \begin{tabular}{|l l|c c c|} 
 \hline
 Подход & $\mathcal{F} $ & STA & chrF1 & STA*chrF1 \\ [0.5ex] 
 \hline
 seq-to-seq & CE & 0.739 & 0.578 & 0.427 \\ 
 GAN style & TP & 0.998 & 0.119 & 0.119 \\ 
 GAN style & $\text{CE} \cdot \text{TP}$  & 0.754 & 0.574 & 0.439 \\
 GAN style & $\text{CE} + 10 \cdot \text{TP}$ & \textbf{0.813} & 0.569 & \textbf{0.462} \\
 \hline
 \end{tabular}
\caption{Что то}
\label{table:final_exp}
\end{table}
