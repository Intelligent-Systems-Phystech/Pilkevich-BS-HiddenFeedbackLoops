@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{gpt3,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  volume={33},
  pages={1877--1901},
  year={2020},
  publisher={Curran Associates, Inc.}
}


@article{gan-bert,
    title={GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples},
    author={Croce, Danilo  and
      Castellucci, Giuseppe  and
      Basili, Roberto},
    booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month={jul},
    year = {2020},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.acl-main.191},
    doi = {10.18653/v1/2020.acl-main.191},
    pages = {2114--2119}
}


@article{kusner2016gans,
  title={Gans for sequences of discrete elements with the gumbel-softmax distribution},
  author={Kusner, Matt J and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal={arXiv preprint arXiv:1611.04051},
  year={2016}
}


@article{gan-wo-rl,
  author    = {David Donahue and
               Anna Rumshisky},
  title     = {Adversarial Text Generation Without Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1810.06640},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.06640},
  eprinttype = {arXiv},
  eprint    = {1810.06640},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-06640.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{popovic-2015-chrf,
    title = {chrF: character n-gram F-score for automatic MT evaluation},
    author = {Popovi{\'c}, Maja},
    month = {sep},
    year = {2015},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/W15-3049},
    doi = {10.18653/v1/W15-3049},
    pages = {392--395},
}


@article{Yu_Zhang_Wang_Yu_2017, title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient}, volume={31}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10804}, abstractNote={ &lt;p&gt; As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong}, year={2017}, month={Feb.} }


@article{Tikhonov2018WhatIW,
  title={What is wrong with style transfer for texts?},
  author={Alexey Tikhonov and Ivan P. Yamshchikov},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.04365}
}


@article{mti5090054,
AUTHOR = {Dementieva, Daryna and Moskovskiy, Daniil and Logacheva, Varvara and Dale, David and Kozlova, Olga and Semenov, Nikita and Panchenko, Alexander},
TITLE = {Methods for Detoxification of Texts for the Russian Language},
JOURNAL = {Multimodal Technologies and Interaction},
VOLUME = {5},
YEAR = {2021},
NUMBER = {9},
ARTICLE-NUMBER = {54},
URL = {https://www.mdpi.com/2414-4088/5/9/54},
ISSN = {2414-4088},
ABSTRACT = {We introduce the first study of the automatic detoxification of Russian texts to combat offensive language. This kind of textual style transfer can be used for processing toxic content on social media or for eliminating toxicity in automatically generated texts. While much work has been done for the English language in this field, there are no works on detoxification for the Russian language. We suggest two types of models—an approach based on BERT architecture that performs local corrections and a supervised approach based on a pretrained GPT-2 language model. We compare these methods with several baselines. In addition, we provide the training datasets and describe the evaluation setup and metrics for automatic and manual evaluation. The results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.},
DOI = {10.3390/mti5090054}
}


@inproceedings{10.5555/2969033.2969125,
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative Adversarial Nets},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2672–2680},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}


@misc{russe_data,
  author = {},
  title = {},
  howpublished = {\url{https://russe.nlpub.org/2022/tox/}},
  year = {}, 
  note = {Russian Text Detoxification Based on Parallel Corpora}
}


@inproceedings{briakou-etal-2021-evaluating,
    title = "Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer",
    author = "Briakou, Eleftheria  and
      Agrawal, Sweta  and
      Tetreault, Joel  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.100",
    doi = "10.18653/v1/2021.emnlp-main.100",
    pages = "1321--1336",
    abstract = "While the field of style transfer (ST) has been growing rapidly, it has been hampered by a lack of standardized practices for automatic evaluation. In this paper, we evaluate leading automatic metrics on the oft-researched task of formality style transfer. Unlike previous evaluations, which focus solely on English, we expand our focus to Brazilian-Portuguese, French, and Italian, making this work the first multilingual evaluation of metrics in ST. We outline best practices for automatic evaluation in (formality) style transfer and identify several models that correlate well with human judgments and are robust across languages. We hope that this work will help accelerate development in ST, where human evaluation is often challenging to collect.",
}


@inproceedings{logacheva-etal-2022-study,
    title = "A Study on Manual and Automatic Evaluation for Text Style Transfer: The Case of Detoxification",
    author = "Logacheva, Varvara  and
      Dementieva, Daryna  and
      Krotova, Irina  and
      Fenogenova, Alena  and
      Nikishina, Irina  and
      Shavrina, Tatiana  and
      Panchenko, Alexander",
    booktitle = "Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.humeval-1.8",
    doi = "10.18653/v1/2022.humeval-1.8",
    pages = "90--101",
    abstract = "It is often difficult to reliably evaluate models which generate text. Among them, text style transfer is a particularly difficult to evaluate, because its success depends on a number of parameters.We conduct an evaluation of a large number of models on a detoxification task. We explore the relations between the manual and automatic metrics and find that there is only weak correlation between them, which is dependent on the type of model which generated text. Automatic metrics tend to be less reliable for better-performing models. However, our findings suggest that, ChrF and BertScore metrics can be used as a proxy for human evaluation of text detoxification to some extent.",
}


@inproceedings{bpe,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}